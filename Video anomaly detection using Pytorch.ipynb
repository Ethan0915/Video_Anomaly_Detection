{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCSDADData(data.Dataset):\n",
    "    def __init__(self, root_dir, seq_len=10, time_stride=1, transform=None):\n",
    "        super(UCSDADData, self).__init__()\n",
    "        self.root_dir=root_dir\n",
    "        vids=[d for d in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, d))]\n",
    "        self.samples=[]\n",
    "        #videos number\n",
    "        for d in vids:\n",
    "            #max possible time stride used for data augmentation\n",
    "            for t in range(1, time_stride+1):\n",
    "                #image index 001 ~ 200\n",
    "                for i in range(1,200):\n",
    "                    if i+(seq_len-1)*t>200:\n",
    "                        break\n",
    "                    self.samples.append((os.path.join(self.root_dir, d), range(i,i+(seq_len-1)*t+1, t)))\n",
    "        \n",
    "        self.pil_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor()])\n",
    "        self.tensor_transform = transforms.Compose([\n",
    "            transforms.Normalize(mean=(0.3750352255196134,), std=(0.20129592430286292,))])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = []\n",
    "        pref = self.samples[index][0]\n",
    "        for fr in self.samples[index][1]:\n",
    "            with open(os.path.join(pref, '{0:03d}.tif'.format(fr)), 'rb') as fin:\n",
    "                frame = Image.open(fin).convert('RGB')\n",
    "                frame = self.pil_transform(frame)\n",
    "                frame = self.tensor_transform(frame)\n",
    "                sample.append(frame)\n",
    "        sample = torch.stack(sample, dim=0)\n",
    "        return sample\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        assert hidden_channels%2==0\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_features = 4\n",
    "        \n",
    "        self.padding = int((kernel_size-1)/2)\n",
    "        \n",
    "        self.Wxi = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whi = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxf = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whf = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxc = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whc = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxo = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Who = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        \n",
    "        self.Wci = None\n",
    "        self.Wcf = None\n",
    "        self.Wco = None\n",
    "    \n",
    "    def forward(self, x, h, c):\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c*self.Wci)\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c*self.Wcf)\n",
    "        cc = cf*c + ci*torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc*self.Wco)\n",
    "        ch = co*torch.tanh(cc)\n",
    "        return ch, cc\n",
    "        \n",
    "    def init_hidden(self, batch_size, hidden, shape, use_cuda=False):\n",
    "        if self.Wci is None:\n",
    "            self.Wci = Variable(torch.zeros(1, hidden, shape[0], shape[1]))\n",
    "            self.Wcf = Variable(torch.zeros(1, hidden, shape[0], shape[1]))\n",
    "            self.Wco = Variable(torch.zeros(1, hidden, shape[0], shape[1]))\n",
    "        else:\n",
    "            assert shape[0]==self.Wci.size()[2], 'Input Height Mismatch!'\n",
    "            assert shape[1]==self.Wci.size()[3], 'Input Width Mismatch!' \n",
    "        if use_cuda:\n",
    "            self.Wci = self.Wci.cuda()\n",
    "            self.Wcf = self.Wcf.cuda()\n",
    "            self.Wco = self.Wco.cuda()\n",
    "        h=Variable(torch.zeros(batch_size, hidden, shape[0], shape[1]))\n",
    "        c=Variable(torch.zeros(batch_size, hidden, shape[0], shape[1]))\n",
    "        if use_cuda:\n",
    "            h,c = h.cuda(), c.cuda()\n",
    "        return (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels,\n",
    "                hidden_channels,\n",
    "                kernel_size,\n",
    "                batch_first=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_channels=[input_channels]+hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = len(hidden_channels)\n",
    "        self.batch_first = batch_first\n",
    "        self._all_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            name = 'cell{}'.format(i)\n",
    "            cell = ConvLSTMCell(self.input_channels[i],self.hidden_channels[i],self.kernel_size)\n",
    "            setattr(self, name, cell)\n",
    "            self._all_layers.append(cell)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if not self.batch_first:\n",
    "            input = input.permute(1,0,2,3,4) #batch first\n",
    "        internal_state=[]\n",
    "        outputs=[]\n",
    "        n_steps = input.size(1) #seq length\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            x = input[:,t,:,:,:]\n",
    "            for i in range(self.num_layers):\n",
    "                name = 'cell{}'.format(i)\n",
    "                if t==0:\n",
    "                    bsize,_,height,width=x.size()\n",
    "                    (h,c)=getattr(self, name).init_hidden(batch_size=bsize,\n",
    "                                                         hidden=self.hidden_channels[i],\n",
    "                                                         shape=(height, width), use_cuda=input.is_cuda)\n",
    "                    internal_state.append((h,c))\n",
    "                \n",
    "                (h,c)=internal_state[i]\n",
    "                x, new_c = getattr(self, name)(x, h, c)\n",
    "                internal_state[i] = (x, new_c)\n",
    "            \n",
    "            outputs.append(x)\n",
    "        outputs=torch.stack(outputs, dim=1)\n",
    "        \n",
    "        return outputs, (x, new_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class VideoAELSTM(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(VideoAELSTM, self).__init__()\n",
    "        self.in_channels=in_channels\n",
    "        self.conv_encoder=nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=self.in_channels, out_channels=128, kernel_size=11, stride=4, padding=0)),\n",
    "            ('nonl1', nn.Tanh()),\n",
    "            ('conv2', nn.Conv2d(in_channels=128, out_channels=64, kernel_size=5, stride=2,padding=0)),\n",
    "            ('nonl2', nn.Tanh())\n",
    "        ]))\n",
    "        self.rnn_enc_dec = ConvLSTM(input_channels=64,\n",
    "                                   hidden_channels=[64,32,64],\n",
    "                                   kernel_size=3,\n",
    "                                   batch_first=True)\n",
    "        self.conv_decoder = nn.Sequential(OrderedDict([\n",
    "            ('deconv1', nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=0)),\n",
    "            ('nonl1', nn.Tanh()),\n",
    "            ('deconv2', nn.ConvTranspose2d(in_channels=128, out_channels=self.in_channels, kernel_size=11, stride=4, padding=0)),\n",
    "            ('nonl2', nn.Tanh())\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.size()\n",
    "        x = x.view(b*t, c, h, w)\n",
    "        x = self.conv_encoder(x)\n",
    "        x = x.view(b, t, x.size(1), x.size(2), x.size(3))\n",
    "        x, _ = self.rnn_enc_dec(x)\n",
    "        x = x.view(b*t, x.size(2), x.size(3), x.size(4))\n",
    "        x = self.conv_decoder(x)\n",
    "        x = x.view(b, t, x.size(1), x.size(2), x.size(3))\n",
    "        return x \n",
    "    def set_cuda(self):\n",
    "        self.conv_encoder.cuda()\n",
    "        self.rnn_enc_dec.cuda()\n",
    "        self.conv_decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='./UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train'\n",
    "train_ds = UCSDADData(root_dir, time_stride=1)\n",
    "train_dl = data.DataLoader(train_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoAELSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e04, eps=1e-6, weight_decay=1e-5)\n",
    "\n",
    "use_cuda=torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    cudnn.benchmark = True\n",
    "    model.set_cuda()\n",
    "    criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iter 0: Loss = 1.0079936981201172\n",
      "Epoch 0, iter 20: Loss = 2.1018834114074707\n",
      "Epoch 0, iter 40: Loss = 2.068948745727539\n",
      "Epoch 0, iter 60: Loss = 1.8993468284606934\n",
      "Epoch 0, iter 80: Loss = 2.0192394256591797\n",
      "Epoch 0, iter 100: Loss = 2.0169758796691895\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for batch_idx, x in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        y=model(x)\n",
    "        loss = criterion(y, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx%20==0:\n",
    "            print('Epoch {}, iter {}: Loss = {}'.format(epoch, batch_idx, loss.item()))\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()},\n",
    "        './snapshot/checkpoint.epoch{}.pth.tar'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoAELSTM()\n",
    "model.load_state_dict(torch.load('./snapshot/checkpoint.epoch4.pth.tar')['state_dict'])\n",
    "#model.set_cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir='./UCSD_Anomaly_Dataset.v1p2/UCSDped1/Inference'\n",
    "test_ds = UCSDADData(test_dir)\n",
    "test_dl = data.DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "errors = []\n",
    "for batch_idx, x in enumerate(test_dl):\n",
    "    y = model(x)\n",
    "    mse = torch.norm(x.cpu().data.view(x.size(0), -1)-y.cpu().data.view(y.size(0),-1), dim=1)\n",
    "    errors.append(mse)\n",
    "errors=torch.cat(errors).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors= errors.reshape(-1,191)\n",
    "s=np.zeros(2,191)\n",
    "s[0, :]=1-(errors[0, :]-np.min(errors[0,:]))/(np.max(errors[0,:] - np.min(errors[0,:])))\n",
    "s[1, :]=1-(errors[1, :]-np.min(errors[0,:]))/(np.max(errors[1,:] - np.min(errors[1,:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test001\n",
    "plt.plot(s[0,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test032\n",
    "plt.plot(s[1,:])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
